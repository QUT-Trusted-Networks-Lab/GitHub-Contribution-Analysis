Hash,Commit Message,Author Name,Author Email,Committor Name,Committor Email,Author Date,Author Timezone,Committor Date,Committor Timezone,in_main_branch,merge,modified_files,parents,deletions,insertions,lines,files,dmm_unit_size,dmm_unit_complexity,dmm_unit_interfacing
7b1ef0ecd602ec371f413767b6fd3b7abf64fa22,Initial commit,Roland Zimmermann,FlashTek@users.noreply.github.com,GitHub,noreply@github.com,2018-06-10 18:21:48+02:00,-7200,2018-06-10 18:21:48+02:00,-7200,True,False,"['.gitignore', 'LICENSE', 'README.md']",[],0,127,127,3,,,
53c89e9d8e75b0c2cef43d08ba01180870c92d8b,Intial commit,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-11 00:15:05+02:00,-7200,2018-06-11 00:15:05+02:00,-7200,True,False,"['README.md', '__init__.py', 'attention.py', 'layer_normalization.py']",['7b1ef0ecd602ec371f413767b6fd3b7abf64fa22'],2,347,349,4,0.47804878048780486,0.824390243902439,0.7951219512195122
78c466335621de249e5832ef605b832aca2feaaa,"Added AttentionRNNWrapper layer

-allows one to wrap attention directly around a RNN to apply the
attention during the temporal evolution of the RNN and not just
afterwards (as SequenceAttention) does",FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-12 00:04:25+02:00,-7200,2018-06-12 00:04:28+02:00,-7200,True,False,['attention.py'],['53c89e9d8e75b0c2cef43d08ba01180870c92d8b'],2,105,107,1,0.6142857142857143,0.6142857142857143,0.38571428571428573
4fd82f6ebd481dee5c6d3172b9381755746c69b7,Updated README,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-12 00:27:07+02:00,-7200,2018-06-12 00:27:07+02:00,-7200,True,False,['README.md'],['78c466335621de249e5832ef605b832aca2feaaa'],3,27,30,1,,,
c7e3cd251e702825e642176dc97cee8f233472e6,Added additive attention to SequenceAttention,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-13 00:16:51+02:00,-7200,2018-06-13 00:16:51+02:00,-7200,True,False,['attention.py'],['4fd82f6ebd481dee5c6d3172b9381755746c69b7'],15,39,54,1,0.46153846153846156,1.0,0.6923076923076923
0dd997486ea42f96fb7fbe0bb3e8678a6f223ac9,Fixed imports,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-13 18:21:25+02:00,-7200,2018-06-13 18:21:34+02:00,-7200,True,False,['layer_normalization.py'],['c7e3cd251e702825e642176dc97cee8f233472e6'],1,1,2,1,,,
5e377ab0988079c7940577090a45af66d1d76d4a,Fixed some problems in the Attention modules,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-13 18:23:12+02:00,-7200,2018-06-13 18:23:12+02:00,-7200,True,False,['attention.py'],['0dd997486ea42f96fb7fbe0bb3e8678a6f223ac9'],20,55,75,1,0.0,0.0,0.0
796e34a0f07f844bc23c7453a040af87891a08eb,Added demo jupyter notebook,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-13 18:24:17+02:00,-7200,2018-06-13 18:24:17+02:00,-7200,True,False,['Attention Demo.ipynb'],['5e377ab0988079c7940577090a45af66d1d76d4a'],0,379,379,1,,,
a74ff97ad18292227ac1b94f1ec3080334ad3124,Renamed the project,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-15 08:22:20+02:00,-7200,2018-06-15 08:22:20+02:00,-7200,True,False,"['README.md', 'Attention Demo.ipynb', '__init__.py', 'attention.py', 'layer_normalization.py']",['796e34a0f07f844bc23c7453a040af87891a08eb'],10,9,19,5,,,
60f68fe1f37aad8c2127aac07f924b7da1bd5fec,Added two layers to resize images,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-17 23:10:30+02:00,-7200,2018-06-17 23:10:30+02:00,-7200,True,False,['image.py'],['a74ff97ad18292227ac1b94f1ec3080334ad3124'],0,170,170,1,0.7361111111111112,1.0,0.5972222222222222
9641112646788322eb306a214d5ddf0ed30f87ac,"Added ExternalAttentionRNNWrapper as described in the Show, Attend, Tell paper",FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-26 19:08:11+02:00,-7200,2018-06-26 19:08:11+02:00,-7200,True,False,['attention.py'],['60f68fe1f37aad8c2127aac07f924b7da1bd5fec'],1,285,286,1,0.32620320855614976,0.5401069518716578,0.35294117647058826
d7b219620923e1c1dbf2117eee0d71fa62bbe882,Added pip package,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-28 06:04:18+02:00,-7200,2018-06-28 06:04:18+02:00,-7200,True,False,"['__version__.py', 'setup.py']",['9641112646788322eb306a214d5ddf0ed30f87ac'],0,118,118,2,1.0,1.0,1.0
e594992cb9ec481b08bcd26edc536ae6956b3ba2,Added pip package,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-28 06:04:18+02:00,-7200,2018-06-28 06:06:07+02:00,-7200,True,False,"['__version__.py', 'setup.py']",['9641112646788322eb306a214d5ddf0ed30f87ac'],0,118,118,2,1.0,1.0,1.0
64d03d509e6512ccccc005da429a3c667e034691,Renamed the project,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-06-28 06:18:09+02:00,-7200,2018-06-28 06:18:09+02:00,-7200,True,False,"['README.md', 'Attention Demo.ipynb', '__init__.py', '__version__.py', 'attention.py', 'image.py', 'layer_normalization.py', 'setup.py']",['f531a613cf750d6b316e515b6d2d5031fc59b403'],5,5,10,8,,,
b3ce1a883314a254fe977eafe441c1afa282587c,"ExternalAttentionRNNWrapper improved

-layer can be saved/loaded now
-attention values can be returned",FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-07-01 22:33:49+02:00,-7200,2018-07-01 22:33:49+02:00,-7200,True,False,['attention.py'],['64d03d509e6512ccccc005da429a3c667e034691'],135,38,173,1,0.76,0.48,0.6666666666666666
728af7cd2fd4499ee00d05e99b0461b17233794e,Loaded ExternalAttentionRNNWrapper objects use the initial_state,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-07-02 22:40:51+02:00,-7200,2018-07-02 22:40:51+02:00,-7200,True,False,['attention.py'],['b3ce1a883314a254fe977eafe441c1afa282587c'],7,107,114,1,0.35384615384615387,0.35384615384615387,0.13846153846153847
98190c47ae0c0e160e8b93c03bb60328d96d8fe9,Fixed wrong input_spec,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-07-04 00:26:05+02:00,-7200,2018-07-04 00:26:05+02:00,-7200,True,False,"['__version__.py', 'attention.py']",['728af7cd2fd4499ee00d05e99b0461b17233794e'],5,6,11,2,0.0,1.0,1.0
0e81fb81c4d23eaff21703ec717787048270d941,AttentionWrappers can now be restored,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-07-05 23:25:30+02:00,-7200,2018-07-05 23:25:30+02:00,-7200,True,False,['attention.py'],['98190c47ae0c0e160e8b93c03bb60328d96d8fe9'],10,21,31,1,1.0,1.0,1.0
d02af8e389e3c3e4d5a64919eabdc39c02e71545,Increased version,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-07-05 23:26:02+02:00,-7200,2018-07-05 23:26:02+02:00,-7200,True,False,['__version__.py'],['0e81fb81c4d23eaff21703ec717787048270d941'],1,1,2,1,,,
f295a3994c8aee9938aa5d5d446945b8a34dc076,LayerNormalization now works not only on sequences with a fixed length,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-07-07 10:18:52+02:00,-7200,2018-07-07 10:18:52+02:00,-7200,True,False,"['attention.py', 'layer_normalization.py']",['d02af8e389e3c3e4d5a64919eabdc39c02e71545'],23,14,37,2,0.0,0.0,0.0
95f82980616df060bc826e53a141976720580179,increased version,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-07-07 10:27:26+02:00,-7200,2018-07-07 10:27:26+02:00,-7200,True,False,"['README.md', '__version__.py']",['f295a3994c8aee9938aa5d5d446945b8a34dc076'],2,10,12,2,,,
a81fcf9cf505e965a3880e44c785d3ee78364370,Improved initial_state handling in AttentionWrapper classes,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-07-11 19:19:36+02:00,-7200,2018-07-11 19:19:36+02:00,-7200,True,False,"['__version__.py', 'attention.py']",['95f82980616df060bc826e53a141976720580179'],20,72,92,2,0.5641025641025641,0.5897435897435898,0.5384615384615384
9e3e597d93daf5aed7ba7ea397d025688f1b3312,Added masking to ScaledDotProductAttention,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-07-16 22:25:38+02:00,-7200,2018-07-16 22:25:38+02:00,-7200,True,False,['attention.py'],['a81fcf9cf505e965a3880e44c785d3ee78364370'],30,36,66,1,0.0,0.0,0.0
3392d42090aa79d1e4cdf56f50ddb571d2d17f25,Updated version number,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-08-29 23:10:01+08:00,-28800,2018-08-29 23:10:01+08:00,-28800,True,False,['__version__.py'],['9e3e597d93daf5aed7ba7ea397d025688f1b3312'],1,1,2,1,,,
4ac0e4353024bddb7f633dc235795b20ed7f01ed,fixed LayerNormalization,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-08-30 23:43:03+08:00,-28800,2018-08-30 23:43:30+08:00,-28800,True,False,"['__version__.py', 'layer_normalization.py']",['3392d42090aa79d1e4cdf56f50ddb571d2d17f25'],6,6,12,2,,,
e0a888277ee0121b88c8541ff7642e4615a76ce1,Fixed tf.Dimension issue,FlashTek,rzrolandzimmermann@gmail.com,FlashTek,rzrolandzimmermann@gmail.com,2018-09-02 23:20:59+08:00,-28800,2018-09-02 23:20:59+08:00,-28800,True,False,"['__version__.py', 'attention.py']",['4ac0e4353024bddb7f633dc235795b20ed7f01ed'],1,6,7,2,0.0,0.0,1.0
a0220b82debcce3f84e86ad6a9c52532171797ec,Update attention.py,Alok singh,43724310+alokssingh@users.noreply.github.com,GitHub,noreply@github.com,2020-05-16 20:40:59+05:30,-19800,2020-05-16 20:40:59+05:30,-19800,True,False,['attention.py'],['e0a888277ee0121b88c8541ff7642e4615a76ce1'],1,2,3,1,,,
